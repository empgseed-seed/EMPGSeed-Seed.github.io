<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="EMPG: Entropy-Modulated Policy Gradient for LLM Agents in Long-Horizon Tasks.">
  <meta name="keywords" content="EMPG, LLM, Agent, Reinforcement Learning, Policy Gradient">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EMPG: Entropy-Modulated Policy Gradient for LLM Agents</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.24/dist/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-slider@2.0.5/dist/css/bulma-slider.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <style>
      :root {
          --empg-primary: #3273dc; /* Bulma's default primary color */
          --empg-light: #f5f5f5;
      }
      .publication-title {
        font-size: 2.5rem;
      }
      .hero.teaser {
        background-color: var(--empg-light);
      }
      .hero-body .subtitle {
        margin-top: 1.5rem;
      }
      .data-card {
        text-align: center;
        padding: 1.5rem;
        background-color: white;
        border-radius: 8px;
        box-shadow: 0 4px 14px rgba(0,0,0,0.05);
        margin-bottom: 1rem;
      }
      .data-card .title {
        font-size: 2.5rem;
        color: var(--empg-primary);
      }
      .data-card .heading {
        font-size: 1rem;
        text-transform: uppercase;
        letter-spacing: 1px;
      }
      .section-title {
        margin-bottom: 3rem;
      }
      .icon-list li {
        display: flex;
        align-items: flex-start;
        margin-bottom: 1rem;
      }
      .icon-list .icon {
        margin-right: 1rem;
        color: var(--empg-primary);
      }
      .footer {
        padding: 3rem 1.5rem 3rem;
      }
      .stat-highlight {
        color: var(--empg-primary);
        font-weight: bold;
      }
      .task-example-img {
        max-width: 100%;
        border: 1px solid #ddd;
        border-radius: 6px;
      }

      /* Styles for Image Modal */
      .task-example-img.zoomable {
        cursor: pointer;
        transition: opacity 0.3s;
      }
      .task-example-img.zoomable:hover {
        opacity: 0.7;
      }
      .modal.image-modal {
        display: none; /* Hidden by default */
        position: fixed;
        z-index: 1000;
        padding-top: 60px;
        left: 0;
        top: 0;
        width: 100%;
        height: 100%;
        overflow: auto;
        background-color: rgb(0,0,0); /* Solid black background */
      }
      .modal-content.image-modal-content {
        margin: auto;
        display: block;
        width: 90%;
        max-width: 1200px;
        animation-name: zoom;
        animation-duration: 0.6s;
      }
      @keyframes zoom {
        from {transform:scale(0.1)}
        to {transform:scale(1)}
      }
      .modal-close {
        position: absolute;
        top: 15px;
        right: 35px;
        color: #f1f1f1;
        font-size: 40px;
        font-weight: bold;
        transition: 0.3s;
      }
      .modal-close:hover,
      .modal-close:focus {
        color: #bbb;
        text-decoration: none;
        cursor: pointer;
      }
      .modal-caption {
        margin: auto;
        display: block;
        width: 80%;
        max-width: 700px;
        text-align: center;
        color: #ccc;
        padding: 10px 0;
        height: 150px;
      }
      .author-block, .contribution-block {
          display: inline-block;
      }
      .equation {
        font-family: 'Castoro', serif;
        font-size: 1.2rem;
        text-align: center;
        margin: 2rem 0;
        padding: 1rem;
        background-color: #fff;
        border-radius: 6px;
        box-shadow: 0 2px 8px rgba(0,0,0,0.05);
      }
      .table-container {
        overflow-x: auto;
      }
      @media only screen and (max-width: 768px){
        .modal-content.image-modal-content {
          width: 100%;
        }
      }
  </style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/js/all.min.js"></script>
  <script>
    $(document).ready(function() {
      // Navbar burger toggle
      $(".navbar-burger").click(function() {
        $(".navbar-burger").toggleClass("is-active");
        $(".navbar-menu").toggleClass("is-active");
      });
    });
  </script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a class="navbar-item" href="#">
      <strong class="is-size-4">EMPG</strong>
    </a>
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="#overview">Overview</a>
      <a class="navbar-item" href="#motivation">Motivation</a>
      <a class="navbar-item" href="#method">Method</a>
      <a class="navbar-item" href="#results">Results</a>
      <a class="navbar-item" href="#paper">Paper</a>
    </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Entropy-Modulated Policy Gradient for LLM Agents in Long-Horizon Tasks</h1>
          <div class="is-size-5 publication-authors" style="margin-top: 1rem;">
            <span class="author-block">Jiawei Wang<sup>1,2,*</sup>,</span>
            <span class="author-block">Jiacai Liu<sup>1,3,*</sup>,</span>
            <span class="author-block">Yuqian Fu<sup>1,4,*</sup>,</span>
            <span class="author-block">Yingru Li<sup>1</sup>,</span>
            <span class="author-block">Yuan Lin<sup>1</sup>,</span>
            <span class="author-block">Yu Yue<sup>1</sup>,</span>
            <span class="author-block">Lin Zhang<sup>1</sup>,</span>
            <span class="author-block">Xintao Wang<sup>1,3</sup>,</span>
            <span class="author-block">Yang Wang<sup>1,†</sup>,</span>
            <span class="author-block">Ke Wang<sup>1,†</sup></span>
          </div>

          <div class="is-size-5 publication-authors" style="margin-top: 0.5rem;">
            <span class="author-block"><sup>1</sup>ByteDance Seed,</span>
            <span class="author-block"><sup>2</sup>University of Science and Technology of China,</span>
            <span class="author-block"><sup>3</sup>Fudan University,</span>
            <span class="author-block"><sup>4</sup>Institute of Automation, Chinese Academy of Sciences</span>
          </div>
          <div class="is-size-6 publication-authors" style="margin-top: 0.5rem;">
              <span class="contribution-block"><sup>*</sup>Work done at ByteDance Seed,</span>
              <span class="contribution-block"><sup>†</sup>Corresponding authors</span>
          </div>
          
          <div class="column has-text-centered" style="margin-top: 1rem;">
            <div class="publication-links">
              <span class="link-block">
                <a href="#paper" class="button is-primary is-rounded is-large">
                  <span class="icon"><i class="fas fa-file-pdf"></i></span>
                  <span>Read the Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/" target="_blank" class="button is-light is-rounded is-large">
                  <span class="icon"><i class="fab fa-github"></i></span>
                  <span>Code (Coming Soon)</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light" id="overview">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <h2 class="title is-3 has-text-centered section-title">Abstract</h2>
            <div class="columns is-centered">
                <div class="column is-10">
                    <div class="content">
                        <p>
                            Large Language Model (LLM) agents face a significant challenge in long-horizon tasks due to sparse, outcome-based rewards that make credit assignment for intermediate steps difficult. We identify a fundamental problem in the learning dynamics itself: a policy gradient's magnitude is inherently coupled with its entropy, leading to inefficiently small updates for confident correct actions and potentially destabilizing large updates for uncertain ones. To resolve this, we propose <strong>Entropy-Modulated Policy Gradients (EMPG)</strong>, a framework that re-calibrates the learning signal based on step-wise uncertainty and the final task outcome. EMPG amplifies updates for confident correct actions, strongly penalizes confident errors to combat "hallucinated confidence," and attenuates updates from uncertain steps to stabilize exploration. We further introduce a "future clarity" intrinsic bonus that encourages the agent to find more predictable solution paths. Through comprehensive experiments on challenging benchmarks like WebShop, ALFWorld, and Deep Search, we demonstrate that EMPG achieves substantial performance gains and significantly outperforms strong policy gradient baselines.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section" id="motivation">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered section-title">Theoretical Motivation</h2>
      <div class="columns is-centered">
          <div class="column is-10">
              <div class="content">
                  <p>
                      Our approach is motivated by a fundamental analysis of the relationship between a policy's gradient and its predictive uncertainty. Standard policy gradients possess an inherent dynamic where high-entropy (uncertain) actions naturally produce large gradients, while low-entropy (confident) actions produce small ones. This presents a dual challenge:
                  </p>
                  <ol>
                      <li>Confident and correct steps, which should be strongly reinforced, receive small updates, limiting learning speed.</li>
                      <li>Uncertain exploratory steps can introduce large, noisy gradients that destabilize training.</li>
                  </ol>
                  <p>
                      This dynamic is formally characterized by the following proposition:
                  </p>
                  <div class="box">
                      <strong>Proposition 1.</strong> For a softmax policy, the expected squared L2-norm of the score function is a direct function of the policy's Rényi-2 entropy, H<sub>2</sub>(&pi;):
                      <div class="equation">
                        E<sub>a~&pi;</sub>[||&nabla;<sub>z</sub> log &pi;(a|s)||<sup>2</sup>] = 1 - exp(-H<sub>2</sub>(&pi;))
                      </div>
                  </div>
                  <p>
                      This shows that the expected gradient norm is monotonically coupled with policy entropy. Our goal is to re-calibrate this learning signal. EMPG provides a two-part re-calibration:
                  </p>
                  <h4 class="title is-4 mt-5">1. Self-Calibrating Gradient Scaling</h4>
                  <p>This component directly addresses the issue by re-calibrating the magnitude of the update based on current-step uncertainty, amplifying reliable signals and attenuating noisy ones.</p>
                  
                  <h4 class="title is-4 mt-5">2. Future Clarity Bonus</h4>
                  <p>
                    However, re-calibrating the update magnitude is only half the solution. A truly effective learning signal must also guide the agent in a useful direction. This motivates our second component, the Future Clarity Bonus, which can be formally justified through the lens of information theory. By providing an intrinsic motivation for the agent to seek low-entropy next states, the bonus encourages actions that yield high Information Gain about the optimal future path. This corresponds to a local, step-wise objective of minimizing the policy's entropy at the next state:
                  </p>
                  <div class="equation">
                    min H(&pi;<sub>&theta;</sub>(&middot;|s<sub>t+1</sub>))
                  </div>
                  <p>This objective imbues the agent with a generalizable meta-skill: to actively seek clarity in the face of ambiguity.</p>
              </div>
          </div>
      </div>
    </div>
  </section>

<section class="section hero is-light" id="method">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered section-title">The EMPG Framework</h2>
      <div class="columns is-vcentered">
        <div class="column is-7">
          <div class="content">
            <h3>Entropy-Modulated Credit Assignment</h3>
            <p>
                Standard policy gradients treat all steps in a trajectory equally, which is inefficient. High-entropy (uncertain) actions produce large gradients, while low-entropy (confident) actions produce small ones. This can destabilize training and slow down learning on correct, confident steps.
            </p>
            <p>
                EMPG solves this by modulating the credit assignment based on step-wise entropy. For a <strong>successful trajectory</strong>, it amplifies updates for confident steps and attenuates updates for uncertain ones, accelerating learning. For a <strong>failed trajectory</strong>, it strongly penalizes confident errors to fight "hallucinated confidence" while avoiding harsh penalties for uncertain exploration.
            </p>
            <h3>Future Clarity Bonus</h3>
            <p>
                EMPG also introduces a "future clarity" bonus. This intrinsic reward encourages the agent to take actions that lead to less uncertain (lower entropy) subsequent states. This guides the agent towards more predictable and stable solution paths, improving overall performance and reliability.
            </p>
          </div>
        </div>
        <div class="column is-5">
            <figure class="image">
                <img src="static/images/overview.png" data-large-src="static/images/overview.png" alt="Conceptual diagram contrasting uniform credit assignment with EMPG's entropy-modulated approach." class="task-example-img zoomable">
                <figcaption class="has-text-centered is-size-7 mt-2">EMPG's confidence-modulated credit assignment vs. baseline. (Click to enlarge)</figcaption>
            </figure>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="results">
    <div class="container is-max-desktop">
        <h2 class="title is-3 has-text-centered section-title">Experimental Results</h2>
        <p class="subtitle is-5 has-text-centered">
            EMPG consistently outperforms strong policy gradient baselines across challenging long-horizon benchmarks.
        </p>

        <div class="columns is-centered is-vcentered" style="margin-top: 2rem;">
            <div class="column is-10">
                <h3 class="title is-4 has-text-centered">Performance on ALFWorld and WebShop</h3>
                <figure class="image">
                    <img src="static/images/alfworld_webshop_exp.png" data-large-src="static/images/alfworld_webshop_exp.png" alt="Table of results for ALFWorld and WebShop benchmarks." class="task-example-img zoomable">
                    <figcaption class="has-text-centered is-size-7 mt-2">Results on ALFWorld and WebShop. (Click to enlarge)</figcaption>
                </figure>
            </div>
        </div>

        <div class="columns is-centered is-vcentered" style="margin-top: 4rem;">
            <div class="column is-10">
                <h3 class="title is-4 has-text-centered">Performance on Deep Search</h3>
                <figure class="image">
                    <img src="static/images/deepsearch_exp.png" data-large-src="static/images/deepsearch_exp.png" alt="Table of results for the Deep Search benchmark." class="task-example-img zoomable">
                     <figcaption class="has-text-centered is-size-7 mt-2">Results on Deep Search. (Click to enlarge)</figcaption>
                </figure>
            </div>
        </div>


        <h3 class="title is-4 has-text-centered" style="margin-top: 4rem;">Analysis</h3>
        <div class="columns is-vcentered is-centered" style="margin-top: 2rem;">
            <div class="column is-5">
                <h4 class="title is-5 has-text-centered">Training Stability</h4>
                <figure class="image">
                    <img src="static/images/kl_loss_stability_comparison.png" data-large-src="static/images/kl_loss_stability_comparison.png" alt="KL Loss dynamics during training for the Qwen2.5-32B-Instruct model." class="task-example-img zoomable"> 
                </figure>
                <p class="is-size-7 mt-2 has-text-centered">The baseline agent (orange) suffers from late-stage instability, while the EMPG-enhanced agent (blue) remains stable throughout training. (Click to enlarge)</p>
            </div>
            <div class="column is-5">
                <h4 class="title is-5 has-text-centered">Step-Level Entropy Dynamics</h4>
                <figure class="image">
                    <img src="static/images/entropy_change_by_bin.png" data-large-src="static/images/entropy_change_by_bin.png" alt="Average entropy change after RL fine-tuning for each entropy percentile." class="task-example-img zoomable">
                </figure>
                <p class="is-size-7 mt-2 has-text-centered">Unlike token-level findings, even low-entropy steps undergo significant changes, validating our step-level analysis approach. (Click to enlarge)</p>
            </div>
        </div>
    </div>
</section>

<section class="section hero is-light" id="paper">
  <div class="container is-max-desktop content">
    <h2 class="title has-text-centered section-title">Paper & Citation</h2>
    <div class="columns is-centered">
      <div class="column is-10">
        <div class="box">
          <p class="has-text-centered">For a detailed description of the method, experiments, and analysis, please refer to our paper.</p>
          <div class="has-text-centered mt-4">
            <a href="https://arxiv.org" class="button is-primary is-rounded">
              <span class="icon"><i class="fas fa-file-pdf"></i></span>
              <span>Read on arXiv (Coming Soon)</span>
            </a>
          </div>
        </div>

        <h3 class="title is-4 mt-6">BibTeX</h3>
        <pre><code>@misc{wang2025empg,
      title={Entropy-Modulated Policy Gradient for LLM Agents in Long-Horizon Tasks}, 
      author={Jiawei Wang and Jiacai Liu and Yuqian Fu and Yingru Li and Yuan Lin and Yu Yue and Lin Zhang and Xintao Wang and Yang Wang and Ke Wang},
      year={2025},
      eprint={your-arxiv-id},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}</code></pre>
      </div>
    </div>
  </div>
</section>

<div id="imageModal" class="modal image-modal">
  <span class="modal-close">&times;</span>
  <img class="modal-content image-modal-content" id="modalImage">
  <div id="modalCaption" class="modal-caption"></div>
</div>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        This website is licensed under a <a rel="license"
                                              href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>.
      </p>
      <p>
        The website template was adapted from the <a
          href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> project page.
      </p>
       <p>
        Contact: <a href="mailto:wangyang.127@bytedance.com">wangyang.127@bytedance.com</a> and <a href="mailto:wangke@bytedance.com">wangke@bytedance.com</a>
      </p>
    </div>
  </div>
</footer>

<script>
// This script handles the image modal functionality
document.addEventListener('DOMContentLoaded', () => {
  // --- Image Modal Functionality ---
  const modal = document.getElementById('imageModal');
  const modalImg = document.getElementById('modalImage');
  const captionText = document.getElementById('modalCaption');
  const closeBtn = document.querySelector('.modal-close');
  const zoomableImages = document.querySelectorAll('.zoomable');

  zoomableImages.forEach(img => {
    img.addEventListener('click', () => {
      modal.style.display = 'block';
      modalImg.src = img.getAttribute('data-large-src') || img.src;
      captionText.innerHTML = img.alt;
    });
  });

  const closeModal = () => {
    modal.style.display = 'none';
  };

  closeBtn.addEventListener('click', closeModal);
  modal.addEventListener('click', (event) => {
    if (event.target === modal) {
      closeModal();
    }
  });
  document.addEventListener('keydown', (event) => {
    if (event.key === 'Escape' && modal.style.display === 'block') {
      closeModal();
    }
  });
});
</script>

</body>
</html>
